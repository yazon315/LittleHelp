{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"nXgNeP5qBe7i"},"outputs":[],"source":["# импорт библиотек\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from IPython.display import display"]},{"cell_type":"markdown","metadata":{"id":"xMTVirWqLoSk"},"source":["# Справка по машинному обучению (ML)\n","\n","Существуют задачи:\n","- классификации;\n","- регрессии.\n","\n","Они определяются видом целевого признака (ответа).  \n","Если целевой признак (ответ) категориальный, то решается задача классификации.  \n","Если целевой признак (ответ) количественный , то решается задача регрессии."]},{"cell_type":"markdown","metadata":{"id":"uBZsTi-zCmbS"},"source":[]},{"cell_type":"markdown","metadata":{"id":"0p5HFwPKBe7r"},"source":["## Алгоритмы машинного обучения (мachine learning algorithms)\n","\n","Существует три типа алгоритмов машинного обучения:\n","- *Обучение с учителем* (spervised machine learning).\n","- *Обучение буз учителя* (unsupervised machine learning).\n","- *Машинное обучение с подкреплением* (reinforcement machine learning).\n","\n","### Виды алгоритмов\n","\n","- **Regression Algorithms**\n","    - [**Linear Regression**](https://towardsdatascience.com/linear-regression-using-least-squares-a4c3456e8570)\n","    - [**Logistic Regression**](https://medium.com/data-science-group-iitr/logistic-regression-simplified-9b4efe801389)\n","    - [**Stepwise Regression**](https://en.wikipedia.org/wiki/Stepwise_regression)  \n","    Это [метод подстройки моделей линейной регрессии](https://en.wikipedia.org/wiki/Stepwise_regression), в ходе которого подбирают имеющиеся входные параметры, перебирая их по одному и оценивая влияние на качество модели. Если параметр улучшает модель, то принимается в набор, если нет, то отвергается.\n","    - [**Multivariate Adaptive Regression Splines (MARS)**](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline)  \n","    Учитывает нелинейность за счет того, что создает точки перегиба (шарниры) между которыми функция линейна. По внешнему виду [напоминает ломаную линию](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline#/media/File:Friedmans_mars_simple_model.png). Точки перегиба (шарниры) высчитывает автоматически. [Плюсы и минусы использования MARS (по ссылке).](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline#Pros_and_cons \"MARS\")\n","    - [**Locally Estimated Scatterplot Smoothing (LOESS)**](https://towardsdatascience.com/loess-373d43b03564)\n","\n","- **Instance-Based Algorithms**\n","    - [**k-Nearest Neighbor (kNN)**]( https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761)\n","    - [**Self-Organizing Map (SOM)**]( https://towardsdatascience.com/self-organizing-maps-ff5853a118d4)\n","    - [**Support Vector Machines (SVM)**]( https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496)\n","\n","- **Clustering Algorithms**\n","\t- [**k-Means**](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1)\n","\t- [**Expectation Maximisation (EM)**]( https://medium.com/@chloebee/the-em-algorithm-explained-52182dbb19d9)\n","\t- [**Hierarchical Clustering**](https://www.kdnuggets.com/2019/09/hierarchical-clustering.html)\n","\n","- **Bayesian Algorithms**\n","\t- [**Naive Bayes**](https://towardsdatascience.com/naive-bayes-explained-9d2b96f4a9c0)\n","\t- [**Gaussian Naive Bayes**](https://medium.com/@LSchultebraucks/gaussian-naive-bayes-19156306079b)\n","\t- [**Averaged One-Dependence Estimators (AODE)**]( https://en.wikipedia.org/wiki/Averaged_one-dependence_estimators)\n","\t- [**Bayesian Network (BN)**]( https://towardsdatascience.com/basics-of-bayesian-network-79435e11ae7b)\n","\t- [**Bayesian Belief Network (BBN)**]( https://www.probabilisticworld.com/bayesian-belief-networks-part-1/)\n","\n","- **Decision Tree Algorithms**\n","\t- [**Conditional Decision Trees**](https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb)\n","\t- [**Classification and Regression Tree (CART)**]( https://www.digitalvidya.com/blog/classification-and-regression-trees/)\n","\t- [**Iterative Dichotomiser 3 (ID3)**]( https://towardsdatascience.com/decision-trees-introduction-id3-8447fd5213e9)\n","\t- [**C4.5 and C5.0**](https://towardsdatascience.com/what-is-the-c4-5-algorithm-and-how-does-it-work-2b971a9e7db0)\n","\n","- **Regularization Algorithms**\n","\t- [**Ridge Regression**](https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db)\n","\t- [**Least Absolute Shrinkage and Selection Operator (LASSO)**]( https://medium.com/@alielagrebi/regularization-lasso-ridge-regression-105f426b749c)\n","\t- [**Elastic Net**](https://medium.com/@vijay.swamy1/lasso-versus-ridge-versus-elastic-net-1d57cfc64b58)\n","\t- [**Least-Angle Regression (LARS)**]( https://medium.com/acing-ai/what-is-least-angle-regression-lar-bb86756f01d0)\n","\n","- **Association Rule Learning Algorithms**\n","\t- [**Apriori algorithm**](https://www.digitalvidya.com/blog/apriori-algorithms-in-data-mining/)\n","\t- [**Eclat algorithm**](https://medium.com/machine-learning-researcher/association-rule-apriori-and-eclat-algorithm-4e963fa972a4)\n","\n","- **Ensemble Algorithms**\n","\t- [**Random Forest**](https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76)\n","\t- [**Boosting**](https://medium.com/greyatom/a-quick-guide-to-boosting-in-ml-acf7c1585cb5)\n","\t- [**Bootstrapped Aggregation (Bagging)**]( https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)\n","\t- [**AdaBoost**](https://towardsdatascience.com/understanding-adaboost-2f94f22d5bfe)\n","\t- [**Weighted Average (Blending)**]( https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/)\n","\t- [**Stacked Generalization (Stacking)**]( https://medium.com/weightsandbiases/an-introduction-to-model-ensembling-63effc2ca4b3)\n","\t- [**Gradient Boosting Machines (GBM)**]( https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab)\n","\t- [**Gradient Boosted Regression Trees (GBRT)**]( https://www.youtube.com/watch?v=3CC4N4z3GJc)\n","\n","- **Artificial Neural Network Algorithms**\n","\t- [**Perceptron**](https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53)\n","\t- [**Multilayer Perceptrons (MLP)**]( https://medium.com/@AI_with_Kain/understanding-of-multilayer-perceptron-mlp-8f179c4a135f)\n","\t- [**Back-Propagation**](https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd)\n","\t- [**Stochastic Gradient Descent**](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31)\n","\t- [**Hopfield Network**](https://medium.com/@serbanliviu/hopfield-nets-and-the-brain-e5880070cdba)\n","\t- [**Radial Basis Function Network (RBFN)**]( https://towardsdatascience.com/radial-basis-functions-neural-networks-all-we-need-to-know-9a88cc053448)\n","\n","- **Deep Learning Neural Network Algorithms**\n","\t- [**Convolutional Neural Network (CNN)**]( https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac)\n","\t- [**Recurrent Neural Networks (RNNs)**]( https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7)\n","\t- [**Long Short-Term Memory Networks (LSTMs)**]( https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n","\t- [**Stacked Auto-Encoders**](https://medium.com/@venkatakrishna.jonnalagadda/sparse-stacked-and-variational-autoencoder-efe5bfe73b64)\n","\t- [**Deep Boltzmann Machine (DBM)**]( https://towardsdatascience.com/restricted-boltzmann-machines-simplified-eab1e5878976)\n","\t- [**Deep Belief Networks (DBN)**]( https://medium.com/analytics-army/deep-belief-networks-an-introduction-1d52bb867a25)\n","\t- [**Generative Adversarial Networks (GANs)**]( https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f)\n","\n","- **Dimensionality Reduction Algorithms**\n","\t- [**Principal Component Analysis (PCA)**]( https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c)\n","\t- [**Principal Component Regression (PCR)**]( https://en.wikipedia.org/wiki/Principal_component_regression)\n","\t- [**Partial Least Squares Regression (PLSR)**]( https://en.wikipedia.org/wiki/Partial_least_squares_regression)\n","\t- [**Linear Discriminant Analysis (LDA)**]( https://medium.com/@srishtisawla/linear-discriminant-analysis-d38decf48105)\n","\t- [**Sammon Mapping**](https://iq.opengenus.org/principle-of-sammon-mapping/)\n","\t- [**Multidimensional Scaling (MDS)**]( https://medium.com/datadriveninvestor/the-multidimensional-scaling-mds-algorithm-for-dimensionality-reduction-9211f7fa5345)\n","\t- [**Projection Pursuit**](https://towardsdatascience.com/interesting-projections-where-pca-fails-fe64ddca73e6)\n"]},{"cell_type":"markdown","metadata":{"id":"3FCuTzyU8iSW"},"source":["## Деление исходных данных на выборки для обучения, тестирования и валидации\n","\n","Два варианта деления исходных данных на выборки\n","\n","Вариант 1:\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=19zp6g67gRcwAzDVhdAEUuVsaJiwSn_Zp\" alt=\"вариант 1\" width=\"60%\" align=\"left\"/>\n","<br clear=\"left\">\n","\n","Вариант 2:\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1A0Bm9UuVyOFbiinyAtB5DGjfSkRtC-tl\" alt=\"вариант 2\" width=\"60%\" align=\"left\" align=\"left\"/>\n","<br clear=\"left\">"]},{"cell_type":"markdown","metadata":{"id":"DhtN3DW3erjp"},"source":["### Функция train_test_split() для деления выборки на части\n","\n","``` python\n","# импортирование функции\n","from sklearn.model_selection import train_test_split\n","# применение функции\n","df_train, df_valid = train_test_split(df, test_size=0.25, random_state=12345)\n","```\n","\n","``` python\n","# еще один пример\n","features_train, features_valid_test, target_train, target_valid_test = train_test_split(\n","  features, target, test_size=.4, random_state=5639, stratify=target)\n","```\n","\n","Функция `train_test_split()` возвращает два новых набора данных: обучающий и валидационный (тестовый).  \n","Параметры функции:\n","- `df` - название набора, данные которого делим;\n","- `test_size` - размер валидационной (или тестовой) выборки, выражается в долях: от 0 до 1 (в этом примере `0.25`, т.к. отделяем 25% исходных данных;\n","- `random_state` - любое значение , но не `None`;\n","- `stratify` - сохраняет соотношение (баланс) между классами целевого признака. Значение `target` - это образец разделения классов в целевом признаке."]},{"cell_type":"markdown","metadata":{"id":"SNMCisdl2C6t"},"source":["## Модели для задач классификации"]},{"cell_type":"markdown","metadata":{"id":"tPWTCEwdEpcf"},"source":["### Метрики качества моделей для задач классификации\n","\n","В библиотеке `sklearn` метрики находятся в модуле `slearn.metrics`."]},{"cell_type":"markdown","metadata":{"id":"6XwZr8GPn2F-"},"source":["#### Confusion matrix (*матрица ошибок или матрица неточностей*)\n","\n","По главной диагонали (от верхнего левого угла) выстроены правильные прогнозы (TN в левом верхнем углу; TP в правом нижнем углу), вне главной диагонали — ошибочные варианты (FP в правом верхнем углу; FN в левом нижнем углу).\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1DtrH2trTa--9xiAyu773ZapoF5_72Fjo\" alt=\"Матрица ошибок\" width=\"30%\" align=\"left\"/>\n","<br clear=\"left\">\n","\n","Класс с меткой «1» называется положительным, с меткой «0» — отрицательным.  \n","- True Positive (TP) - истинно положительные ответы;\n","- True Negative (TN) - истинно отрицательные ответы;\n","- False Positive (FP) - ложноположительные ответы;\n","- False Negative (FN) - ложноотрицательные ответы.\n","\n","Функция `confusion_matrix()` принимает на вход верные ответы и предсказания модели, а возвращает матрицу ошибок."]},{"cell_type":"markdown","metadata":{"id":"lUkJhXzMmMch"},"source":["#### 1) **Accuracy** (*доля правильных ответов*): отношение числа правильных ответов к размеру тестовой выборки.\n","\n","$\\begin{aligned}\n","accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\n","\\end{aligned}$\n","\n","Интуитивно понятная, очевидная и почти неиспользуемая метрика. Чем *Accuracy* больше, тем точнее модель. Но эта метрика бесполезна в задачах с неравными классами (дисбаланс классов).  \n","Вычисляется функцией `accuracy_score()`. Функция принимает на вход два аргумента: правильные ответы и предсказания модели; возвращает значение *Accuracy*.\n","\n","``` python\n","from sklearn.metrics import accuracy_score\n","accuracy = accuracy_score(target, predictions)\n","```"]},{"cell_type":"markdown","metadata":{"id":"2dIzQswgmNk4"},"source":["#### 2) **Precision** (*точность*): показывает, какая доля объектов, для которой модель поставила целевой признак, действительно имеют целевой признак.\n","\n","$\\begin{aligned}\n","precision = \\frac{TP}{TP+FP}\n","\\end{aligned}$\n","\n","Точность определяет, как много отрицательных ответов нашла модель, пока искала положительные. Чем больше отрицательных, тем ниже точность.  \n","Вычисляется функцией `precision_score()`.\n","\n","``` python\n","from sklearn.metrics import precision_score\n","print(precision_score(target_valid, predicted_valid))\n","```"]},{"cell_type":"markdown","metadata":{"id":"-5BSW2FOmN63"},"source":["#### 3) **Recall** (*полнота*): выявляет, какую часть объектов, имеющих целевой признак, правильно определила модель.\n","\n","$\\begin{aligned}\n","recall = \\frac{TP}{TP+FN}\n","\\end{aligned}$\n","\n","Полнота — это доля TP-ответов среди всех, у которых истинная метка 1. Хорошо, когда значение *Recall* близко к единице: модель хорошо ищет положительные объекты. Если полнота ближе к нулю — модель надо перепроверить и починить.  \n","Вычисляется функцией `recall_score()`.\n","\n","``` python\n","from sklearn.metrics import recall_score\n","print(recall_score(target_valid, predicted_valid))\n","```"]},{"cell_type":"markdown","metadata":{"id":"fjvCYR-amosb"},"source":["#### 4) **F1-score** (*F1-мера*): агрегирующая метрика — среднее гармоническое полноты и точности.\n","\n","$\\begin{aligned}\n","F1\\text{-}score=\\frac{2 * precision * recall}{precision + recall}\n","\\end{aligned}$\n","\n","Если положительный класс плохо прогнозируется по одной из шкал (*Recall* или *Precision*), то близкая к нулю *F1-мера* покажет, что прогноз класса 1 не удался.  \n","Вычисляется функцией `f1_score()`.\n","\n","``` python\n","from sklearn.metrics import f1_score\n","print(f1_score(target_valid, predicted_valid))\n","```"]},{"cell_type":"markdown","metadata":{"id":"j5AQfdmUZw8u"},"source":["#### 5) **PR-кривая** (от англ. *Precision и Recall*)\n","\n","Показывает на графике значение точности `precision` и полноты `recall` при изменении порога. Чем выше кривая, тем лучше модель.\n","\n","Порог - граница, где заканчивается отрицательный класс и начинается положительный. По умолчанию он равен 0,5, но его можно поменять.\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1FskmNXpBQfhVmHVu3N23fCweU38B9kVv\" alt=\"PR-кривая\" width=\"40%\" align=\"left\"/>\n","<br clear=\"left\">\n","\n","Вероятность классов вычисляет функция `predict_proba()`. На вход она получает признаки объектов, а возвращает вероятности классов.\n","``` python\n","probabilities = model.predict_proba(features)\n","print(probabilities)\n","```\n","результат:\n","``` python\n","[[0.5795 0.4205]\n"," [0.6629 0.3371]\n"," [0.7313 0.2687]\n"," [0.6728 0.3272]\n"," [0.5086 0.4914]] \n","```"]},{"cell_type":"markdown","metadata":{"id":"qhymVcCkgJqu"},"source":["#### 6) **TPR и FPR**\n","\n","Когда положительных объектов нет, точность не вычислить, но есть другие характеристики, в которых нет деления на ноль.\n","\n","**TPR** (англ. *True Positive Rate*) или «полнота», на английском используют термин *recall* - это доля верно предсказанных объектов к общему числу объектов класса.\n","\n","$\\begin{aligned}\n","TPR = \\frac{TP}{TP+FN}\n","\\end{aligned}$\n","\n","**FPR** (англ. *False Positive Rate*) - это доля ложных срабатываний к общему числу объектов за пределами класса.  \n","Отношение FP-ответов (*False Positives* — отрицательные, классифицированные как положительные) к сумме отрицательных ответов FP и TN (*True Negatives* — верно классифицированные отрицательные ответы).\n","\n","Деления на ноль не будет, т.к. в знаменателях значения, которые постоянны и не зависят от изменения модели.\n","\n","$\\begin{aligned}\n","FPR = \\frac{FP}{FP+TN}\n","\\end{aligned}$"]},{"cell_type":"markdown","metadata":{"id":"yLKLKXC1jTMa"},"source":["#### 7) **ROC-кривая** или **кривая ошибок** (от англ. *receiver operating characteristic*) и **AUC-ROC** (от англ. *Area Under Curve ROC* - «*площадь под ROC-кривой*»)\n","\n","Для модели, которая всегда отвечает случайно, ROC-кривая выглядит как прямая, идущая из левого нижнего угла в верхний правый. Чем график выше, тем больше значение TPR и лучше качество модели.\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1FufAXL2wVglvlGH4mls17VXXlxrjTsVr\" alt=\"ROC-кривая\" width=\"60%\" align=\"left\"/>\n","<br clear=\"left\">\n","\n","Строят ROC-кривую с помощью функции `roc_curve()`. Она принимает на вход значения целевого признака и вероятности положительного класса, перебирает разные пороги и возвращает три списка: значения FPR, значения TPR и рассмотренные пороги.\n","\n","``` python\n","from sklearn.metrics import roc_curve\n","# создание и обучение модели\n","model = LogisticRegression(random_state=12345, solver='liblinear')\n","model.fit(features_train, target_train)\n","# прогнозирование вероятностей классов и выделение класса \"1\"\n","probabilities_valid = model.predict_proba(features_valid)\n","probabilities_one_valid = probabilities_valid[:, 1]\n","# получение параметров для ROC-кривой\n","fpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid)\n","# постороение ROC-кривой\n","plt.figure()\n","plt.plot(fpr, tpr)\n","# ROC-кривая случайной модели (выглядит как прямая)\n","plt.plot([0, 1], [0, 1], linestyle='--')\n","# параметры графика\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.0])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC-кривая')\n","plt.show()\n","```\n","\n","Чтобы выявить, на сколько сильно модель отличается от случайной, определяют **AUC-ROC** - площадь под ROC-кривой.  \n","Эта метрика качества изменяется от 0 до 1. AUC-ROC случайной модели равна 0.5. Вычисляется с помощью функции `roc_auc_score()`.\n","\n","``` python\n","from sklearn.metrics import roc_auc_score\n","auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n","```"]},{"cell_type":"markdown","metadata":{"id":"zddVmlSYT6H0"},"source":["### Модель \"дерево решений\" для задач классификации"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PefngpZfdoLt"},"outputs":[],"source":["import pandas as pd\n","\n","# импортирование модели \"дерево решений\"\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# загружаем исходный датасет\n","df = pd.read_csv('train_data.csv')\n","\n","# отдельно определили, что медиана выборки (средняя стоимость квартиры)\n","# равна 5650000 рублей\n","# создаем новый категориальный столбец ('price_class') - это целевая переменная\n","# для данной задачи, если \"дорого\", то 'price_class'=1,\n","# если \"дешево\", то 'price_class'=0\n","df.loc[df['last_price'] > 5650000, 'price_class'] = 1\n","df.loc[df['last_price'] <= 5650000, 'price_class'] = 0\n","\n","# делим исходный датасет на два:\n","# 1) один с параметрами (features), от которых зависит целевая переменная\n","# в нем не должно быть целевой переменной 'price_class')\n","# и в данной задаче не нужна цена квартиры, т.к. вместо неё мы ввели целевую\n","# переменную 'price_class'\n","# 2) второй с целевой переменной (target)\n","features = df.drop(['last_price', 'price_class'], axis=1)\n","target = df['price_class']\n","\n","# создаем модель типа \"дерево решений\"\n","model = DecisionTreeClassifier(random_state=12345)\n","\n","# обучаем модель на двух заполненных выборках с параметрами и целевой переменной\n","model.fit(features, target)\n","\n","# создаем два новых объекта с параметрами для тестирования модели\n","new_features = pd.DataFrame(\n","    [[900, 12, 2.8, 25, 409.7, 25, 0, 0, 0, 112, 0, 30706.0, 7877.0],\n","     [109, 2, 2.75, 25, 32, 25, 0, 0, 0, 40.5, 0, 36421.0, 9176.0]],\n","    columns=features.columns)\n","\n","# предсказываем ответы, загружая параметры двух новых тестовых объектов в модель\n","answers = model.predict(new_features)\n","# печатаем результат\n","print(answers)"]},{"cell_type":"markdown","metadata":{"id":"OsUIssQdoD0q"},"source":["### Глубина обучения\n","\n","**Переобучение** означает, что модель плохо поняла зависимости в данных. Модель хорошо объясняет примеры из обучающего набора данных, но плохо отрабатывает на тестовой выборке (значение параметра accuracy на тестовой выборке модели ниже, чем на обучающей).\n","\n","**Недообучение** - обратно переобучению, возникает, когда качество на обучающей и тестовой выборках примерно одинаковое и низкое.\n","\n","**Глубина дерева** (высота дерева) — это максимальное количество условий от «вершины» до финального ответа (считается по количеству переходов между узлами). Если дерево высокое (большое количество переходов между узлами/условиями), у модели склонность к переобучению; низкое — к недообучению. Решающее дерево с одним переходом (вопросом) называют \"пнем\" :). \n","\n","Глубина дерева (глубина обучения) в sklearn задаётся параметром max_depth.  \n","``` python\n","model = DecisionTreeClassifier(random_state=12345, max_depth=3)\n","```"]},{"cell_type":"markdown","metadata":{"id":"mnKjNjB7AK34"},"source":["### Модель \"случайный лес\" для задач классификации\n","\n","Алгоритм обучает большое количество независимых друг от друга деревьев, а потом принимает решение на основе голосования. Случайный лес помогает улучшить результат предсказания и избежать переобучения."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgYebKjD_6sp"},"outputs":[],"source":["import pandas as pd\n","# импортируем модель \"случайный лес\"\n","from sklearn.ensemble import RandomForestClassifier\n","# импортируем функцию разделения выборки на обучающую и тестовую(валидационную)\n","from sklearn.model_selection import train_test_split\n","# импортируем функцию определения точности предсказаний модели\n","from sklearn.metrics import accuracy_score\n","\n","# загружаем выборку\n","df = pd.read_csv('/datasets/train_data.csv')\n","# добавляем в выборку целевой категорийный признак\n","df.loc[df['last_price'] > 5650000, 'price_class'] = 1\n","df.loc[df['last_price'] <= 5650000, 'price_class'] = 0\n","\n","# делим выборку на валидационную (25%) и обучающую (всё остальное - 75%)\n","# обязательно назначаем параметр random_state\n","# (любое значение, но потом его не меняем)\n","df_train, df_valid = train_test_split(df, test_size=.25, random_state=12345)\n","\n","# делим обучающую и валидационную выборки на наборы параметров\n","# и целевых значений\n","features_train = df_train.drop(['last_price', 'price_class'], axis=1)\n","target_train = df_train['price_class']\n","features_valid = df_valid.drop(['last_price', 'price_class'], axis=1)\n","target_valid = df_valid['price_class']\n","\n","# создаем модель типа \"случайный лес\"\n","# зададим, что в случайном лесу будет 10 деревьев (n_estimators=10)\n","model = RandomForestClassifier(random_state=12345, n_estimators=10)\n","\n","# обучаем модель на обучающей (тренировочной) выборке\n","model.fit(features_train, target_train)\n","\n","# получаем предсказания обученной модели на тестовой выборке\n","predictions = model.predict(features_valid)\n","# точность модели определяем функцией accuracy_score()\n","accuracy = accuracy_score(target_valid, predictions)\n","\n","# или методом .score(), который считает accuracy для всех алгоритмов\n","# классификации (а для всех моделей регрессии этот же метод .score()\n","# вычисляет метрику r2_score), в этом случае не нужен этап расчета\n","# model.predict(), т.к. метод .score() делает этот расчет внутри себя\n","result = model.score(features_valid, target_valid)"]},{"cell_type":"markdown","metadata":{"id":"5M7hlqNKKltr"},"source":["### Модель \"логистическая регрессия\" для задач классификации\n","\n","Не смотря на такое название, это алгоритм для задачи классификации, а не регрессии. Это логистическое уравнение придумал бельгийский математик Франсуа Ферхюльст. В логистической регрессии параметров мало, поэтому вероятность переобучения невелика."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCAme99GK1_k"},"outputs":[],"source":["import pandas as pd\n","from joblib import dump\n","\n","# импортируем модель \"логистическая регрессия\"\n","from sklearn.linear_model import LogisticRegression\n","# импортируем функцию разделения выборки на обучающую и тестовую(валидационную)\n","from sklearn.model_selection import train_test_split\n","\n","# создаем модель типа \"логистической регрессии\"\n","# solver='lbfgs' - определяет алгоритм, который будет строить модель\n","# алгоритм 'lbfgs' из самых распространённых, подходит для большинства задач\n","# max_iter задаёт максимальное количество итераций обучения (по умолчанию 100)\n","model = LogisticRegression(random_state=12345, solver='lbfgs', max_iter=1000)\n","\n","# обучаем модель на обучающей (тренировочной) выборке\n","model.fit(features_train, target_train)\n","\n","# загружаем модель на сервер\n","dump(model, 'model_9_1.joblib')"]},{"cell_type":"markdown","metadata":{"id":"r_D73356bu7f"},"source":["Обучая логистическую регрессию, можно столкнуться с предупреждением библиотеки sklearn. Чтобы его отключить, указывают аргумент `solver='liblinear'` (англ. solver «алгоритм решения»; library linear, «библиотека линейных алгоритмов»):  \n","`model = LogisticRegression(solver='liblinear')`"]},{"cell_type":"markdown","metadata":{"id":"HasbgampRvi5"},"source":["### Сравнение моделей для задач классификации\n","\n","Модель | Качество (accuracy) | Скорость работы\n",":--- | :---: | :---:\n","**Дерево решений**\t| Низкое |\tВысокая\n","**Случайный лес**\t| Высокое |\tНизкая\n","**Логистическая регрессия** |\tСреднее |\tВысокая\n"]},{"cell_type":"markdown","metadata":{"id":"g53tMqX-3KG5"},"source":["## Модели для задач регрессии"]},{"cell_type":"markdown","metadata":{"id":"YFAiQ93TtPbi"},"source":["### Метрики качества моделей для задач регрессии\n"]},{"cell_type":"markdown","metadata":{"id":"s1UYONPZo91F"},"source":["#### 1) **MSE** (англ. *Mean Squared Error*)\n","\n","**Средняя квадратичная ошибка** - наиболее распространённая метрика качества в задаче регрессии. Чем MSE меньше, тем точнее модель.\n","\n","$\\begin{aligned}\n","MSE=\\frac{\\text{Сумма квадратов ошибок объектов}}{\\text{Количество объектов}}\n","\\end{aligned}$\n","\n","или\n","\n","$\\begin{aligned}\n","MSE=\\frac1N\\sum_{i=1}^N(y_i-\\hat{y}_i)^2\n","\\end{aligned}$\n","\n","Функция расчета средней квадратичной ошибки - `mean_squared_error()` из библиотеки `sklearn`.\n","\n","``` python\n","# импортируем функцию расчёта MSE из библиотеки sklearn\n","from sklearn.metrics import mean_squared_error\n","\n","# сформируем для примера правильные ответы и предсказания\n","answers = [623, 253, 150, 237]\n","predictions = [649, 253, 370, 148]\n","\n","# расчет MSE\n","result = mean_squared_error(answers, predictions)\n","```"]},{"cell_type":"markdown","metadata":{"id":"Uh9r6XCFtPvQ"},"source":["#### 2) **RMSE** (англ. *root mean squared error*)\n","\n","**Корень из средней квадратичной ошибки** - это квадратный корень из средней квадратичной ошибки (MSE). Применяют чтобы метрика показывала просто рубли, а не \"квадратные рубли\" как после расчета средней квадратичной ошибки (MSE). \n","\n","$\\begin{aligned}\n","RMSE = \\sqrt{\\frac1N\\sum_{i=1}^N(y_i-\\hat{y}_i)^2}=\\sqrt{MSE}\n","\\end{aligned}$\n","\n","``` python\n","# RMSE рассчитывается как квадратный корень из MSE:\n","result = mean_squared_error(answers, predictions) **.5\n","\n","# или сразу так:\n","result = mean_squared_error(answers, predictions, squared=False)\n","```"]},{"cell_type":"markdown","metadata":{"id":"MDPzMJRpwCsQ"},"source":["#### 3) **метрика R2** (англ. *coefficient of determination; R-squared*)\n","\n","**Коэффициент детерминации** - вычисляет долю средней квадратичной ошибки модели от MSE среднего, а затем вычитает эту величину из единицы. Увеличение метрики означает прирост качества модели.\n","- Значение метрики R2 равно единице только если MSE нулевое. Такая модель предсказывает все ответы идеально.\n","- R2 равно нулю: модель работает так же, как и среднее.\n","- Если метрика R2 отрицательна, то качество модели очень низкое.\n","- Значения R2 больше единицы быть не может.\n","\n","$\\begin{aligned}\n","R^2=1-\\frac{MSE(model)}{MSE(baseline)}\n","\\end{aligned}$\n","\n","Вычисляется функцией `r2_score()`.\n","\n","``` python\n","from sklearn.metrics import r2_score\n","# создание и обучение библиотеки, получение предсказаний\n","model = LinearRegression()\n","model.fit(features_train, target_train)\n","predicted_valid = model.predict(features_valid)\n","# расчет и вывод R2\n","print(\"R2 =\", r2_score(target_valid, predicted_valid))\n","```"]},{"cell_type":"markdown","metadata":{"id":"DmlQU6G8zb2C"},"source":["#### 4) **MAE (англ. mean absolute error)**\n","\n","**Среднее абсолютное отклонение** -  похожа на MSE, но в ней нет возведения в квадрат. Вычисляется функцией `mean_absolute_error()`.\n","\n","$\\begin{aligned}\n","MAE=\\frac1N\\sum_{i=1}^N|y_i-\\hat y_i|\n","\\end{aligned}$\n","\n","``` python\n","from sklearn.metrics import mean_absolute_error\n","print(mean_absolute_error(target_valid, predicted_valid))\n","```\n"," "]},{"cell_type":"markdown","metadata":{"id":"8WfWfmDNxWoD"},"source":["### Модель \"дерево решений\" для регрессии\n","\n","``` python\n","import pandas as pd\n","# импортируем модель \"дерево решений\" для регрессии\n","from sklearn.tree import DecisionTreeRegressor\n","# импортируем функцию разделения выборки на обучающую и тестовую(валидационную)\n","from sklearn.model_selection import train_test_split\n","# импортируем функцию для расчета MSE/RMSE\n","from sklearn.metrics import mean_squared_error\n","\n","# загрузка данных\n","df = pd.read_csv('/datasets/train_data.csv')\n","\n","# разделение исходных данных на набор параметров и набор целевых значений\n","# целевые значения переводим в млн. руб. (разделив на 1000000)\n","features = df.drop(['last_price'], axis=1)\n","target = df['last_price'] / 1000000\n","\n","# делим исходные данные, представленные двумя наборами `features` и `target`,\n","# на обучающую и валидационную выборки (каждая также представляется\n","# двумя наборами)\n","# для валидационной выборки выделяем 25% исходных данных\n","features_train, features_valid, target_train, target_valid = train_test_split(\n","    features, target, test_size=.25, random_state=12345)\n","\n","# в цикле перебираем глубину \"дерева решений\", проверяем качество модели\n","# через RMSE и определяем наилучшую модель (RMSE меньше)\n","best_model = None\n","best_result = 10000\n","best_depth = 0\n","for depth in range(1, 6):\n","    # создаем модель \"дерево решений\" для регрессии\n","    # с заданной глубиной max_depth\n","    model = DecisionTreeRegressor(random_state=12345, max_depth=depth)\n","    # обучаем модель на тренировочной выборке\n","    model.fit(features_train, target_train)\n","    # получаем предсказания модели на валидационной выборке\n","    predictions_valid = model.predict(features_valid)\n","    # считаем значение метрики RMSE на валидационной выборке\n","    result = mean_squared_error(target_valid, predictions_valid) **.5\n","    if result < best_result:\n","        best_model = model\n","        best_result = result\n","        best_depth = depth\n","\n","print(\"RMSE наилучшей модели на валидационной выборке:\", best_result,\n","      \"Глубина дерева:\", best_depth)\n","```"]},{"cell_type":"markdown","metadata":{"id":"mGrkS9PF10mv"},"source":["### Модель \"случайный лес\" для регрессии\n","\n","``` python\n","import pandas as pd\n","# импортируем модель \"случайный лес\" для регрессии\n","from sklearn.ensemble import RandomForestRegressor\n","# импортируем функцию разделения выборки на обучающую (тренировочную)\n","# и валидационную (тестовую)\n","from sklearn.model_selection import train_test_split\n","# импортируем функцию для расчета MSE/RMSE\n","from sklearn.metrics import mean_squared_error\n","\n","# загрузка данных\n","df = pd.read_csv('/datasets/train_data.csv')\n","\n","# разделение исходных данных на набор параметров и набор целевых значений\n","# целевые значения переводим в млн. руб. (разделив на 1000000)\n","features = df.drop(['last_price'], axis=1)\n","target = df['last_price'] / 1000000\n","\n","# делим исходные данные (представленные двумя наборами `features` и `target`)\n","# на обучающую и валидационную выборки (каждая также представляется \n","# двумя наборами)\n","# для валидационной выборки выделяем 25% исходных данных\n","features_train, features_valid, target_train, target_valid = train_test_split(\n","    features, target, test_size=.25, random_state=12345)\n","\n","# в цикле перебираем количество деревьев `est` с шагом 10\n","# и глубину `depth` \"случайного леса\"\n","# проверяем качество модели через RMSE и определяем наилучшую модель\n","# (у которой RMSE меньше)\n","best_model = None\n","best_result = 10000\n","best_est = 0\n","best_depth = 0\n","for est in range(10, 51, 10):\n","    for depth in range (1, 11):\n","        # инициализируем модель \"случайный лес\"\n","        # с количеством деревьев n_estimators=est, \n","        # глубиной max_depth=depth и фиксированным параметром random_state=12345\n","        model = RandomForestRegressor(random_state=12345, n_estimators=est,\n","                                      max_depth=depth)\n","        # обучаем модель на тренировочной выборке\n","        model.fit(features_train, target_train)\n","        # получаем предсказания модели на валидационной выборке\n","        predictions_valid = model.predict(features_valid)\n","        # считаем значение метрики rmse на валидационной выборке\n","        result = mean_squared_error(target_valid, predictions_valid,\n","                                    squared=False)\n","        if result < best_result:\n","            best_model = model\n","            best_result = result\n","            best_est = est\n","            best_depth = depth\n","\n","print(\"RMSE наилучшей модели на валидационной выборке:\", best_result,\n","      \"Количество деревьев:\", best_est, \"Максимальная глубина:\", depth)\n","```"]},{"cell_type":"markdown","metadata":{"id":"a-puokEPUxUv"},"source":["### Модель \"линейная регрессия\" для регрессии\n","\n","``` python\n","import pandas as pd\n","# импортируем модель \"линейная регрессия\" для регрессии\n","from sklearn.linear_model import LinearRegression\n","# импортируем функцию разделения выборки на обучающую (тренировочную)\n","# и валидационную (тестовую)\n","from sklearn.model_selection import train_test_split\n","# импортируем функцию для расчета MSE/RMSE\n","from sklearn.metrics import mean_squared_error\n","\n","# загрузка данных\n","df = pd.read_csv('/datasets/train_data.csv')\n","\n","# разделение исходных данных на набор параметров и набор целевых значений\n","# целевые значения переводим в млн. руб. (разделив на 1000000)\n","features = df.drop(['last_price'], axis=1)\n","target = df['last_price'] / 1000000\n","\n","# делим исходные данные (представленные двумя наборами `features` и `target`)\n","# на обучающую и валидационную выборки (каждая также представляется\n","# двумя наборами)\n","# для валидационной выборки выделяем 25% исходных данных\n","features_train, features_valid, target_train, target_valid = train_test_split(\n","    features, target, test_size=.25, random_state=12345)\n","\n","# инициализируем модель \"линейная регрессия\"\n","model = LinearRegression()\n","# обучаем модель на тренировочной выборке\n","model.fit(features_train, target_train)\n","# получаем предсказания модели на валидационной выборке\n","predictions_valid = model.predict(features_valid)\n","# считаем значение метрики RMSE на валидационной выборке\n","result = mean_squared_error(target_valid, predictions_valid) ** .5\n","\n","print(\"RMSE модели линейной регрессии на валидационной выборке:\", result)\n","```"]},{"cell_type":"markdown","metadata":{"id":"tYINlZLlfiG6"},"source":["## Рекомендации по выбору лучшей модели\n","\n","Иногда (и даже, похоже, всегда) нужно перебирать разные варианты моделей, задавая разные гиперпараметры, сравнивать их результаты и на этом основании выбирать лучшую модель."]},{"cell_type":"markdown","metadata":{"id":"qBu8vEU-hNPE"},"source":["## Подготовка признаков\n","\n","В задачах машинного обучения к анализу готовят не только данные, но ещё и признаки. Модели обучаются на числовых данных, поэтому пропуски в данных и нечисловые значения могут вызвать ошибку и невозможность обучения модели.  \n","(признаки - это данные в выборках `features`, где нет целевого признака)"]},{"cell_type":"markdown","metadata":{"id":"Dj5hbLnCJ9d0"},"source":["### Разделение признаков по видам\n","\n","Для удобства обработки (кодирования, масштабирования и т.п.) формируют списки столбцов признаков по видам, как минимум, делят на категориальные признаки и числовые. Они обрабатываются разными функциями, а после обработки собираются обратно в выборки признаков `features`.  \n","Функции предобработки признаков, также как и модели, обучаются на признаках обучающей выборки, а потом обученные на обучающей выборке функции предобработки применяются к признакам обучающей, валидационной и тестовой выборок.\n","\n","Пример такого формирования списков столбцов с категориальными и численными признаками:\n","\n","``` python\n","# сформируем список категориальных признаков\n","# признаки 'has_cr_card' и 'is_active_member' уже имеют нужный формат\n","# и не требуют обработки\n","list_category = features_train.select_dtypes(include='object').columns.to_list()\n","#list_category.extend(['has_cr_card', 'is_active_member'])\n","print(f'''\n","Список категориальных признаков:\n","{list_category}''')\n","\n","# сформируем список численных признаков\n","list_numeral = features_train.select_dtypes(exclude='object').columns.to_list()\n","# признаки 'has_cr_card' и 'is_active_member' категориальные (бинарные 0/1),\n","# хоть и имеют в датасете тип численных, исключаем их из списка\n","list_numeral.remove('has_cr_card')\n","list_numeral.remove('is_active_member')\n","print(f'''\n","Список численных признаков:\n","{list_numeral}''')\n","```\n","Результат:\n","```\n","Список категориальных признаков:\n","['geography', 'gender']\n","\n","Список численных признаков:\n","['credit_score', 'age', 'tenure', 'balance', 'num_of_products', 'estimated_salary']\n","```"]},{"cell_type":"markdown","metadata":{"id":"0gkkJY2vA7jO"},"source":["### Техника прямого кодирования (англ. *One-Hot Encoding*, *OHE*).\n","\n","Позволяет преобразовать категориальные признаки в численные.  \n","Подходит для моделей \"логистическая реграссия\", \"решающее дерево\", \"случайный лес\".  \n","В частности, принадлежность к категории логистическая регрессия вычисляет по формуле, состоящей из признаков. Они могут быть только численные.\n","\n","Категориальные признаки переводятся в численные в два этапа:\n","1. Для каждого значения признака создаётся новый столбец.\n","2. Если объекту категория подходит, в новом столбце ставится 1, если нет — 0.\n","\n","Новые признаки (новые столбцы) называются *дамми-переменными* или *дамми-признаками* (англ. dummy variable, «фиктивная переменная»).\n","\n","Функция `pd.get_dummies()` из библиотеки `pandas` реализует технику прямого кодирования.\n","\n","*Важно!*  \n","Из всего переданного датафрейма, функция `pd.get_dummies()` обрабатывает только категориальные (нечисловые) столбцы (object). Она сама определяет их по типу данных столбцов (можно посмотреть через `.info()` или `.dtypes`).\n","\n","Пример применения функции `pd.get_dummies()` для столбца \"Gender\" датасета (с распечаткой первых значений):\n","```python\n","print(pd.get_dummies(data['Gender']).head())\n","```\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1BnhsuMdm4tsVHaSUEbKuhzjPXUiuxnG3\" alt=\"Пример применения техники One-Hot Encoding\" width=\"50%\" align=\"left\"/>\n","<br clear=\"left\">"]},{"cell_type":"markdown","metadata":{"id":"uzz4mMp8cNa_"},"source":["При отработке техники прямого кодирования One-Hot Encoding можно столкнуться с так называемой *дамми-ловушкой* (англ. dummy trap, «ловушка фиктивных признаков»), когда столбцы, добавляемые в датасет функцией `pd.get_dummies()`, сильно взаимосвязаны между собой. Это означает избыточность данных, что плохо для обучения.  \n","В примере с обработакой техникой прямого кодирования столбца `'Gender'` это, например, столбец `'Gender_F'`, значения которого однозначно можно определить исходя из значений двух других столбцов `'Gender_M'` и `'Gender_None'`. Соответственно, столбец `'Gender_F'` лишний и его нужно удалить.  \n","Удаление осуществляется с помощью параметра `drop_first=True` функции `pd.get_dummies()`.  \n","Пример:\n","``` python\n","data_ohe = pd.get_dummies(data['Gender'], drop_first=True)\n","```\n","\n","Альтернативный пример реализации техники прямого кодирования.  \n","Использование функции `OneHotEncoder()` (более предпочтительно, чем `pd.get_dummies()`).\n","\n","``` python\n","# отключение предупреждения 'SettingWithCopy'\n","pd.options.mode.chained_assignment = None\n","# кодирование категориальных признаков в обучающей выборке,\n","# имена столбцов в списке 'list_category'\n","encoder = OneHotEncoder(drop='first', sparse_output=False)\n","features_train[encoder.get_feature_names()] = encoder.fit_transform(\n","                                              features_train[list_category])\n","# после обработки добавились новые столбцы, образованные из исходных,\n","# старые столбцы удаляем\n","features_train = features_train.drop(list_category, axis=1)\n","\n","# кодирование категориальных признаков в валидационной выборке энкодером,\n","# который уже обучен на обучающей выборке\n","features_valid[encoder.get_feature_names()] = encoder.transform(\n","                                              features_valid[list_category])\n","# удаляем старые, теперь не нужные столбцы\n","features_valid = features_valid.drop(list_category, axis=1)\n","```"]},{"cell_type":"markdown","metadata":{"id":"5Qd_DnZV1eBl"},"source":["### Техника порядкового кодирования\n","\n","Также преобразует категориальные признаки в численные.  \n","Подходит для моделей \"решающее дерево\" и \"случайный лес\". Для \"логистической регрессии\" не подходит, так как .  \n","Кодирует цифрами выраженные категории в столбцах. Ordinal Encoding (от англ. «кодирование по номеру категории»). Работает следующим образом:\n","- Фиксируется, какой цифрой кодируется класс.\n","- Цифры размещаются в столбце.\n","\n","Функция для применения техники прямого кодирования `OrdinalEncoder()` находится в модуле `sklearn.preprocessing`.\n","\n","*Важно!*  \n","Функция `OrdinalEncoder()` перекодирует все столбцы в переданном ей датасете и категориальные и числовые. Данные в числовых столбцах изменятся!\n","\n","``` python\n","# импортируем OrdinalEncoder из библиотеки\n","from sklearn.preprocessing import OrdinalEncoder\n","\n","# Преобразование выполняется в три этапа:\n","# 1. Создаём объект этой структуры данных\n","encoder = OrdinalEncoder()\n","# 2. Вызываем метод fit(), чтобы получить список категориальных признаков\n","encoder.fit(data)\n","# 3. Преобразуем данные функцией transform()\n","data_ordinal = encoder.transform(data)\n","\n","# преобразуем в датафрейм с названиями столбцов, как в исходном data\n","data_ordinal = pd.DataFrame(encoder.transform(data), columns=data.columns)\n","```\n","\n","``` python\n","# два варианта написания `fit()` и `transform()` в одну строку\n","# (результат одинаковый):\n","# первый\n","data_ordinal = pd.DataFrame(encoder.fit(data).transform(data), columns=data.columns)\n","# второй\n","data_ordinal = pd.DataFrame(encoder.fit_transform(data), columns=data.columns)\n","```\n","<img src=\"https://drive.google.com/uc?export=view&id=1BuFSUAvBx8tY8iyAve-9o9OJjaDJ6rck\" alt=\"Пример применения техники Ordinal Encoding\" width=\"20%\" align=\"left\"/>\n","<br clear=\"left\">\n","\n","Пример кода с применением порядкового кодирования Ordinal Encoding и обучения модели \"решающее дерево\":\n","``` python\n","# импортирование необходимых библиотек и функций\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.preprocessing import OrdinalEncoder\n","# загрузка данных\n","data = pd.read_csv('/datasets/travel_insurance.csv')\n","# применение прямого кодирования\n","encoder = OrdinalEncoder()\n","data_ordinal = pd.DataFrame(encoder.fit_transform(data), columns=data.columns)\n","# деление выборки на обучающую и валидационную\n","target = data_ordinal['Claim']\n","features = data_ordinal.drop('Claim', axis=1)\n","features_train, features_valid, target_train, target_valid = train_test_split(\n","                        features, target, test_size=0.25, random_state=12345)\n","# обучение модели \"решающее дерево\" на обучающей выборке\n","model = DecisionTreeClassifier(random_state=12345)\n","model.fit(features_train, target_train)\n","```\n","\n","Техника порядкового кодирования `Ordinal Encoding` используется реже, чем техника прямого кодирования `One-Hot Encoding`."]},{"cell_type":"markdown","metadata":{"id":"b4hyYpWacyIC"},"source":["### Масштабирование признаков\n","\n","Часто признаки имеют разный масштаб, поэтому их нужно выравнивать - стандартизировывать.  \n","Например, в столбце `Age` возможен возраст от 0 до 100 лет, а в столбце `Commission` страховая комиссия от 100 долларов до 1000. Значения и их разбросы в столбце Commission больше, поэтому алгоритм автоматически решит, что этот признак важнее возраста. А на самом дела для нас все признаки одинаково значимы.  \n","Чтобы избежать этой ловушки, признаки масштабируют — приводят к одному масштабу.\n","Один из методов масштабирования — **стандартизация данных**.\n","\n","Функция `StandardScaler()` из модуля `sklearn.preprocessing` используется для стандартизации данных.\n","``` python\n","# импортирование функции масштабирования данных\n","from sklearn.preprocessing import StandardScaler\n","# создание объекта функции масштабирования данных и настройка его на обучающих данных\n","# (настройка — это вычисление среднего и дисперсии)\n","scaler = StandardScaler()\n","scaler.fit(features_train)\n","# преобразование (масштабирование) обучающей и валидационной выборки функцией `transform()`\n","features_train_scaled = scaler.transform(features_train)\n","features_valid_scaled = scaler.transform(features_valid)\n","```\n","\n","Обучающей (тренировочной) выборке параметров (`features`) можно сразу сделать одной командой `.fit_transform()`, а валидационной и тестовой выборкам параметров (`features`) в любом случае `.transform()` отдельно.\n","\n","При записи изменённых признаков в исходный датафрейм код может вызывать предупреждение `SettingWithCopy`. Причина этого в особенности поведения `sklearn` и `pandas`.  Чтобы предупреждение не появлялось, в код добавляют строчку:\n","``` python\n","pd.options.mode.chained_assignment = None\n","```\n","\n","*Важно!*  \n","масштабирование функцией `StandardScaler()` применяется к числовым данным, поэтому в методах `.fit()` и `.transform()` этой функции надо передавать только столбцы датасета с числовыми данными.\n","\n","Пример кода с применением масштабирования с помощью функции `StandardScaler()`:\n","``` python\n","# импортирование необходимых библиотек\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","# отключение предупреждения, связанного с работой функции `StandardScaler()`\n","pd.options.mode.chained_assignment = None\n","# загрузка данных\n","data = pd.read_csv('/datasets/travel_insurance.csv')\n","# разделение выборки на обучающую и валидационную\n","target = data['Claim']\n","features = data.drop('Claim', axis=1)\n","features_train, features_valid, target_train, target_valid = train_test_split(\n","    features, target, test_size=0.25, random_state=12345)\n","# список числовых столбцов для масштабирования\n","numeric = ['Duration', 'Net Sales', 'Commission (in value)', 'Age']\n","# инициация и обучение функции масштабирования\n","scaler = StandardScaler()\n","scaler.fit(features_train[numeric])\n","# применение функции масштабирования\n","features_train[numeric] = scaler.transform(features_train[numeric])\n","features_valid[numeric] = scaler.transform(features_valid[numeric])\n","# просмотр результатов в обучающей выборке\n","print(features_train.head())\n","```"]},{"cell_type":"markdown","metadata":{"id":"q-MFDaq7Ic8f"},"source":["## Балансирование классов\n","\n","Для балансировки классов применяют следующие техники:\n","- Взвешивание классов.\n","- Изменение размеров выборки (масштабирование или стандартизация):\n","  - увеличение выборки (upsampling);\n","  - уменьшение выборки (downsampling)."]},{"cell_type":"markdown","metadata":{"id":"lECqrUoZR2Nw"},"source":["### Техника взвешивания классов\n","\n","Для балансировки классов при создании моделей используется параметр `class_weight='balanced'`, который есть в функциях создания моделей `DecisionTreeClassifier()`, `RandomForestClassifier()` и `LogisticRegression()`. Он увеличивает вес класса, которого меньше. Например, если \"1\" в 3 раза меньше, чем \"0\", то у \"0\" будет вес 1, а у \"1\" будет вес 3.\n","\n","Пример:\n","\n","``` python\n","model = DecisionTreeClassifier(random_state=12345, max_depth=depth, class_weight='balanced')\n","```"]},{"cell_type":"markdown","metadata":{"id":"UyFbE31VS6y8"},"source":["### Техника \"upsampling\"\n","\n","Преобразование проходит в несколько этапов:\n","- разделить обучающую выборку на отрицательные и положительные объекты;\n","- скопировать несколько раз положительные объекты;\n","- с учётом полученных данных создать новую обучающую выборку;\n","- перемешать данные.\n","\n","Перемешивание данных осуществляется с помощью функции `upsample()`.  \n","Объем выборки после применения \"upsampling\" увеличивается.\n","\n","Пример балансировки выборки в которой \"1\" меньше \"0\" в 4 раза:\n","\n","``` python\n","# делим обучающие выборки на отрицательные и положительные объекты\n","features_zeros = features_train_balans[target_train_balans == 0]\n","features_ones = features_train_balans[target_train_balans == 1]\n","target_zeros = target_train_balans[target_train_balans == 0]\n","target_ones = target_train_balans[target_train_balans == 1]\n","\n","# увеличиваем количество положительных объектов в 4 раза путем копирования\n","features_train_upsampled = pd.concat([features_zeros] + [features_ones] * 4)\n","target_train_upsampled = pd.concat([target_zeros] + [target_ones] * 4)\n","\n","# перемешиваем\n","features_train_upsampled, target_train_upsampled = shuffle(\n","  features_train_upsampled, target_train_upsampled, random_state=12345)\n","```"]},{"cell_type":"markdown","metadata":{"id":"id3lZeNDU5-Z"},"source":["### Техника \"downsampling\"\n","\n","Преобразование проходит в несколько этапов:\n","- разделить обучающую выборку на отрицательные и положительные объекты;\n","- случайным образом отбросить часть из отрицательных объектов;\n","- с учётом полученных данных создать новую обучающую выборку;\n","- перемешать данные.\n","\n","Чтобы удалить из выборки случайные элементы, применяется функция `sample()`. На вход она принимает аргумент `frac`. Возвращает случайные элементы в таком количестве, чтобы их доля от исходной таблицы была равна `frac`.  \n","Объем выборки после применения \"downsampling\" уменьшается.\n","\n","Пример балансировки выборки сокращением доли \"0\" до 0,1:\n","\n","``` python\n","# делим обучающие выборки на отрицательные и положительные объекты\n","features_zeros = features[target == 0]\n","features_ones = features[target == 1]\n","target_zeros = target[target == 0]\n","target_ones = target[target == 1]\n","# сокращаем долю \"0\" в 10 раз, удаляя и оставляя их долю\n","# в размере 0,1 от исходной выборки \"0\"\n","features_downsampled = pd.concat([features_zeros.sample(\n","  frac=.1, random_state=12345)] + [features_ones])\n","target_downsampled = pd.concat([target_zeros.sample(\n","  frac=.1, random_state=12345)] + [target_ones])\n","# перемешиваем\n","features_downsampled, target_downsampled = shuffle(\n","  features_downsampled, target_downsampled, random_state=12345)\n","```"]},{"cell_type":"markdown","metadata":{"id":"CgLryGTABe8A"},"source":["## Разное"]},{"cell_type":"markdown","metadata":{"id":"Dri6j32jBe8B"},"source":["#### Создание случайных выборок для задач линейной регрессии и классификации\n","\n","- **`make_regression`** - генерирует случайный набор для линейной регрессии в виде выборок признаков (features), целевых признаков (target) и коэффициентов линейной регрессии (w). Основные параметры:\n","    - `n_samples` - количество строк в выборках;\n","    - `n_features` - количество признаков (features);\n","    - `n_informative` - количество информативных признаков (?), используемых для построения линейной модели;\n","    - `n_targets` - количество целевых признаков (target);\n","    - `coef` - если `True`, то выдает выборку с коэффициентами линейной регрессии модели (по умолчанию `False`);\n","    - `bias` - задает параметр смещения в базовой модели (по умолчанию 0);\n","    - `noise` - задает отклонение в виде плавающего гаусовского шума (по умолчанию 0);\n","    - `random_state` - задает генератор случайных чисел.\n","\n","```python\n","# импортирование из библиотеки\n","from sklearn.datasets import make_regression\n","# создание выборки\n","X, y, w = make_regression(n_samples=1000, n_features=7, n_informative=4, n_targets=1, coef=True, \n","                          bias=.7, noise=.4, random_state = 12345)\n","```\n","\n","- **`make_classification`** - аналогично генерирует случайный набор для классификации. Основные параметры:\n","    - `n_samples` - количество строк в выборках;\n","    - `n_features` - количество признаков (features);\n","    - `n_informative` - количество информативных признаков (?), используемых для построения линейной модели;\n","    - `n_redundant` - количество избыточных функций (?) (по умолчанию 2);\n","    - `n_classes` - количество классов целевого признака (по умолчанию 2) (должно быть пропорционально `n_informative`);\n","    - `weights=` - задает веса (пропорции) классов (по умолчанию `None` - классы сбалансированны);\n","    - `random_state` - задает генератор случайных чисел.\n","    \n","```python\n","# импортирование из библиотеки\n","from sklearn.datasets import make_classification\n","# создание выборки\n","X, y = make_classification(n_samples=10, n_features=3, n_informative=3, n_redundant=0, n_classes=2, weights=[.2, .8], \n","                           random_state = 12345)\n","```"]},{"cell_type":"markdown","metadata":{},"source":["#### Создание константной модели с помощью `DummyRegressor()`\n","\n","Константная модель создается для оценки адекватности рассматриваемых моделей. Метрики создаваемых в проекте моделей не должны быть хуже метрик константной модели.\n","\n","Создание константной dummy-модели, состоящей из медианных значений, для двух этапов процесса и расчет для неё итогового 'sMAPE'.\n","\n","```python\n","# создание, обучение и получение предсказаний дамми-модели с медианными значениями для чернового концентрата\n","dummy_r = DummyRegressor(strategy='median').fit(features_train_rougher, target_train_rougher)\n","predict_dummy_r = pd.Series(dummy_r.predict(features_test_rougher))\n","\n","# создание, обучение и получение предсказаний дамми-модели с медианными значениями для финального концентрата\n","dummy_f = DummyRegressor(strategy='median').fit(features_train_final, target_train_final)\n","predict_dummy_f = pd.Series(dummy_f.predict(features_test_final))\n","\n","# расчет итогового sMAPE для дамми-модели\n","print('Итоговое sMAPE для дамми-модели:', smape_total(target_test_rougher, predict_dummy_r, target_test_final, predict_dummy_f))\n","```"]},{"cell_type":"markdown","metadata":{},"source":["#### Применение `Pipeline()`, `ColumnTransformer()` с `SimpleImputer()`, `StandardScaler()` и `OneHotEncoder()` и `GridSearchCV()`\n","\n","Обучение модели `RandomForestClassifier` с предварительной подготовкой данных в столбцах:\n","- для числовых столбцов:\n","    - заполнение пропусков медианой с помощью `SimpleImputer`;\n","    - масштабирование с помощью `StandardScaler`;\n","- для категориальных столбцов:\n","    - заполнение пропусков значением 'missing' с помощью `SimpleImputer`;\n","    - кодирование с помощью `OneHotEncoder`.\n","\n","Формируются два `Pipeline` для предподготовки столбцов с численными и категориальными признаками, которые вставляются в `ColumnTransformer`. Затем формируется `Pipeline` из `ColumnTransformer` и `RandomForestClassifier`.\n","\n","```python\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# формирование выборок\n","X = train.drop('Loan_Status', axis=1)\n","y = train['Loan_Status']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","# формирование Pipeline для числовых и категориальных столбцов\n","numeric_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler', StandardScaler())])\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n","\n","# создание списков столбцов с числовыми и категориальными признаками\n","numeric_features = train.select_dtypes(include=['int64', 'float64']).columns\n","categorical_features = train.select_dtypes(include=['object']).drop(['Loan_Status'], axis=1).columns\n","\n","# применение ColumnTransformer с Pipeline для обработки данных\n","# в столбцах с числовыми и категориальными признаками\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)])\n","\n","# обучение и получение предсказаний модели RandomForestClassifier с использованием Pipeline\n","# в который вложен ColumnTransformer со своими двумя Pipeline\n","rf = Pipeline(steps=[('preprocessor', preprocessor),\n","                      ('classifier', RandomForestClassifier())])\n","rf.fit(X_train, y_train)\n","y_pred = rf.predict(X_test)\n","```\n","\n","В цикле отрабатывается `Pipeline`, состоящий из `ColumnTransformer` (описан в предыдущем примере) и модели из списка, обучается модель и рассчитывается её метрика на тестовых данных.\n","\n","```python\n","from sklearn.metrics import accuracy_score, log_loss\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC, LinearSVC, NuSVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","\n","# список моделей\n","classifiers = [\n","    KNeighborsClassifier(3),\n","    SVC(kernel=\"rbf\", C=0.025, probability=True),\n","    NuSVC(probability=True),\n","    DecisionTreeClassifier(),\n","    RandomForestClassifier(),\n","    AdaBoostClassifier(),\n","    GradientBoostingClassifier()\n","    ]\n","\n","# предобработка столбцов и создание модели с помощью Pipeline\n","for classifier in classifiers:\n","    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n","                      ('classifier', classifier)])\n","    # обучение модели, расчет и вывод метрики модели\n","    pipe.fit(X_train, y_train)   \n","    print(classifier)\n","    print(\"model score: %.3f\" % pipe.score(X_test, y_test))\n","```\n","\n","Подбор гиперпараметров для модели `RandomForestClassifier` с помощью `GridSearchCV`. В `GridSearchCV` передается `Pipeline` ('rf'), состоящий из `ColumnTransformer` и `RandomForestClassifier` (описан в предыдущем примере) и решетка (значения) гиперпараметров для перебора.\n","\n","```python\n","from sklearn.model_selection import GridSearchCV\n","\n","param_grid = { \n","    'classifier__n_estimators': [200, 500],\n","    'classifier__max_features': ['auto', 'sqrt', 'log2'],\n","    'classifier__max_depth' : [4,5,6,7,8],\n","    'classifier__criterion' :['gini', 'entropy']}\n","CV = GridSearchCV(rf, param_grid, n_jobs= 1)\n","CV.fit(X_train, y_train)  \n","print(CV.best_params_)    \n","print(CV.best_score_)\n","```"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Двухэтапный подбор гиперпараметров модели `RandomForestRegressor()` с помощью `RandomizedSearchCV()` и `GridSearchCV()`\n","\n","Сначала выполняется подбор гиперпараметров в широком диапазоне с помощью `RandomizedSearchCV`. По результатам строятся графики влияния гиперпараметров на модель с помощью `seaborn.barplot()`. После анализа графиков определяется более узкий диапазон подбора гиперпараметров и передается для окончательного подбора в `GridSearchCV`.  \n","Подбор параметров с помощью `RandomizedSearchCV` и `GridSearchCV` оформлен в виде двух пользовательских функций. В `RandomizedSearchCV` и `GridSearchCV` применяется кроссвалидация и используется пользовательская метрика 'custom_scorer', которая создается с помощью `make_scorer()` и пользовательской функции 'smape_rasch'.  \n","Используется `Pipeline`, состоящий из масштабирования признаков `StandardScaler` и модели `RandomForestRegressor`.  \n","В конце строится график важности признаков для модели (какие признаки в какой стопени были использованы моделью при обучении).\n","\n","```python\n","# функция подбора гиперпараметров для 'RandomForestRegressor' с помощью 'RandomizedSearchCV'\n","# на входе выборки с параметрами и целевым параметром\n","# на выходе модель, лучшее sMAPE и набор лучших гиперпараметров в виде Series\n","def rfr_rscv(features, target):\n","    # создание пайплайн\n","    pipe = Pipeline([\n","        ('scaler', StandardScaler()),\n","        ('regressor', RandomForestRegressor(random_state=RST))\n","        ])\n","    # определение параметров и их диапазона для подбора\n","    param_distr = [{\n","        'regressor__n_estimators': [int(x) for x in np.linspace(start = 10, stop = 300, num = 20)],\n","        'regressor__max_depth': [int(x) for x in np.linspace(start = 1, stop = 15, num = 5)],\n","        'regressor__min_samples_leaf': [int(x) for x in np.linspace(start = 1, stop = 15, num = 5)],\n","        'regressor__min_samples_split': [int(x) for x in np.linspace(start = 2, stop = 15, num = 5)],\n","        'regressor__max_features': ['log2', 'sqrt', 1]\n","        }]\n","    # определение пользовательской метрики sMAPE на базе функции 'smape_rasch'\n","    custom_scorer = make_scorer(smape_rasch, greater_is_better=False)\n","    # подбор гиперпараметров с использованием кросс-валидации\n","    rfr_rs = RandomizedSearchCV(pipe, param_distributions=param_distr, n_iter = 100, scoring=custom_scorer, cv=3, \n","                                verbose=1, n_jobs=-1, random_state=RST)\n","    rfr_rs.fit(features, target)\n","    smape_best = -rfr_rs.best_score_\n","    param_best = pd.Series(rfr_rs.best_params_)\n","    # вывод результатов\n","    print(f'''\n","Модель \"случайный лес\"\n","Лучшее sMAPE: {smape_best}\n","Лучшие гиперпараметры:\n","{param_best}\n","    ''')\n","    return rfr_rs, smape_best, param_best\n","\n","\n","# подбор гиперпараметров для 'RandomForestRegressor' с помощью 'RandomizedSearchCV'\n","print('Этап чернового концентрата:')\n","mod_rfr_rs_r, smape_rfr_rs_r, param_rfr_rs_r = rfr_rscv(features_train_rougher, target_train_rougher)\n","\n","# построение графиков влияния гиперпараметров на модель\n","res_rs = pd.DataFrame(mod_rfr_rs_r.cv_results_).sort_values(by='rank_test_score')\n","fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n","sns.set_style(\"whitegrid\")\n","fig.suptitle('Влияние гиперпараметров на модель')\n","sns.barplot(x='param_regressor__n_estimators', y='mean_test_score', data=res_rs, ax=axes[0,0], color='lightgrey'\n","           ).set_xlabel('')\n","axes[0,0].set_ylim([-9,1])\n","axes[0,0].set_xticklabels(axes[0,0].get_xticklabels(), rotation=45)\n","axes[0,0].set_title(label = 'n_estimators', weight='bold')\n","sns.barplot(x='param_regressor__max_depth', y='mean_test_score', data=res_rs, ax=axes[0,1], color='lightpink'\n","           ).set_xlabel('')\n","axes[0,1].set_ylim([-9,1])\n","axes[0,1].set_title(label = 'max_depth', weight='bold')\n","sns.barplot(x='param_regressor__max_features', y='mean_test_score', data=res_rs, ax=axes[0,2], color='wheat'\n","           ).set_xlabel('')\n","axes[0,2].set_ylim([-9,1])\n","axes[0,2].set_title(label = 'max_features', weight='bold')\n","sns.barplot(x='param_regressor__min_samples_leaf', y='mean_test_score', data=res_rs, ax=axes[1,0], color='lightgreen'\n","           ).set_xlabel('')\n","axes[1,0].set_ylim([-9,1])\n","axes[1,0].set_title(label = 'min_samples_leaf', weight='bold')\n","sns.barplot(x='param_regressor__min_samples_split', y='mean_test_score', data=res_rs, ax=axes[1,1], color='coral'\n","           ).set_xlabel('')\n","axes[1,1].set_ylim([-9,1])\n","axes[1,1].set_title(label = 'min_samples_split', weight='bold')\n","plt.show()\n","\n","\n","# функция подбора гиперпараметров для 'RandomForestRegressor' с помощью 'GridSearchCV'\n","# на входе выборки с параметрами и целевым параметром, диапазоны гиперпараметров\n","# на выходе модель, лучшее sMAPE и набор лучших гиперпараметров в виде Series\n","def rfr_gscv(features, target, n_estimators, max_depth, max_features, min_samples_leaf, min_samples_split):\n","    # создание пайплайн\n","    pipe = Pipeline([\n","        ('scaler', StandardScaler()),\n","        ('regressor', RandomForestRegressor(random_state=RST))\n","        ])\n","    # определение параметров и их диапазона для подбора\n","    param_grid = [{\n","        'regressor__n_estimators': n_estimators,\n","        'regressor__max_depth': max_depth,\n","        'regressor__max_features': max_features,\n","        'regressor__min_samples_leaf': min_samples_leaf,\n","        'regressor__min_samples_split': min_samples_split\n","        }]\n","    # определение пользовательской метрики sMAPE на базе функции 'smape_rasch'\n","    custom_scorer = make_scorer(smape_rasch, greater_is_better=False)\n","    # подбор гиперпараметров с использованием кросс-валидации\n","    rfr_gs = GridSearchCV(pipe, param_grid=param_grid, scoring=custom_scorer, cv=5, verbose=1, n_jobs=-1)\n","    rfr_gs.fit(features, target)\n","    smape_best = -rfr_gs.best_score_\n","    param_best = pd.Series(rfr_gs.best_params_)    \n","    # вывод результатов\n","    print(f'''\n","Модель \"случайный лес\"\n","Лучшее sMAPE: {smape_best}\n","Лучшие гиперпараметры:\n","{param_best}\n","    ''')\n","    return rfr_gs, smape_best, param_best\n","\n","\n","# диапазоны гиперпараметров для подбора в 'RandomForestRegressor'\n","p_n_estimators = [50, 55, 60, 155, 160, 165]\n","p_max_depth = range(10, 12)\n","p_max_features = ['sqrt']\n","p_min_samples_leaf = range (10, 12)\n","p_min_samples_split = [2, 3, 10, 11, 12]\n","\n","# подбор гиперпараметров для 'RandomForestRegressor' с помощью 'GridSearchCV'\n","print('Этап чернового концентрата:')\n","mod_rfr_gs_r, smape_rfr_gs_r, param_rfr_gs_r = rfr_gscv(features_train_rougher, target_train_rougher,\n","                                p_n_estimators, p_max_depth, p_max_features, p_min_samples_leaf, p_min_samples_split)\n","# построение графика важности признаков для модели\n","pd.Series(mod_rfr_gs_r.best_estimator_.named_steps['regressor'].feature_importances_, features_train_rougher.columns\n","         ).sort_values(ascending=False).plot(kind='bar', figsize=(16,5), title='Важность признаков для модели')\n","plt.show()\n","```"]},{"cell_type":"markdown","metadata":{},"source":["#### Визуализация `Pipeline`, `ColumnTransformer`, `GridSearchCV` и т.п.\n","\n","Переводит отображение в вид раскрывающихся структурных схем.\n","\n","```python\n","import sklearn\n","sklearn.set_config(display='diagram')\n","pipe\n","```"]},{"cell_type":"markdown","metadata":{"id":"fGalOYC6Be8B"},"source":["#### Визуализация модели дерева\n","\n","Позволяет отобразить структуру построения узлов модели типа \"дерево\" (в примере модель находится в переменной `tree_1`). Параметры:\n","- `max_depth` - отображаемая на рисунке глубина дерева (по умолчанию `None` - всё дерево);\n","- `feature_names` - можно вручную задать названия признаков модели для отображения на рисунке;\n","- `filled` - в случае `True` делает закраску узлов в зависимости от значения или категории;\n","- `node_ids` - в случае `True` отображает номер узла;\n","- `proportion` - в случае `True` отображает процент значений, отработанных узлов, в случае `None` или отсутствия отображает количество значений, отработанных узлом;\n","- `rounded` - в случае `True` скругляет углы у линий и меняет шрифт;\n","- `precision` - определяет количество знаков после запятой у значений (по умолчанию 3).\n","\n","```python\n","# импортирование из бибилиотеки\n","from sklearn.tree import plot_tree\n","# отрисовка\n","plt.figure(figsize=(15,10))\n","plot_tree(tree_1, max_depth=5, feature_names=['feature_1', 'feature_2', 'feature_3'], filled=True, node_ids=True, \n","          proportion=True, rounded=True, precision=2)\n","plt.show()\n","```"]},{"cell_type":"markdown","metadata":{"id":"pubibjl6uLyA"},"source":["#### Библиотека `lazypredict`\n","\n","Позволяет оценить, какая модель лучше отработает на имеющихся данных. Содержит 42 модели из Scikit-learn Parameters с нерегулируемыми гиперпараметрами, которые запускаются на обучающей и тестовой выборках одной командой. В качестве результата выдает рассчитанные метрики и предсказания для каждого вида моделей в формате DataFrame. Есть два набора: для задач регрессии `LazyRegressor` и задач классификации `LazyClassifier`. Можно использовать пользовательские метрики, предварительно импортировав их из Scikit-learn, например: `custom_metric=r2_score`.\n","\n","Параметры:\n","- `verbose` (по умолчанию `0`) - For the 'liblinear' and 'lbfgs' solvers set verbose to any positive number for verbosity;\n","- `ignore_warnings` (по умолчанию `True`) - если установлено значение `True`, игнорируются предупреждения, связанные с алгоритмами;\n","- `custom_metric` (по умолчанию `None`) - если функция предоставляется, модели оцениваются на основе пользовательской метрики;\n","- `prediction` (по умолчанию `False`) - если установлено `True`, то прогнозы моделей возвращаются в виде DataFrame;\n","- `regressors` (для `LazyRegressor`) - (по умолчанию `all`) - When function is provided, trains the chosen regressor(s).\n","- `classifiers` (для `LazyClassifier`) - (по умолчанию `all`) - When function is provided, trains the chosen classifier(s).\n","\n","```python\n","# скачивание библиотеки 'lazypredict'\n","pip install lazypredict\n","# импортирование библиотеки и наборов для задач регрессии и классификации\n","import lazypredict\n","from lazypredict.Supervised import LazyRegressor\n","from lazypredict.Supervised import LazyClassifier\n","# создание набора моделей для задач регрессии\n","mult_ML_model = LazyRegressor(verbose=0, ignore_warnings=True, predictions=True, random_state=12345, )\n","models, predictions = mult_ML_model.fit(X_train, X_test, y_train, y_test)\n","model_dictionary = mult_ML_model.provide_models(X_train, X_test, y_train, y_test)\n","# отображение рассчитанных метрик моделей\n","display(models.sort_values(by='RMSE'))\n","# отображение предсказаний моделей\n","display(predictions)\n","# отображение pipeline и команд вызова отрабатываемых моделей\n","display(model_dictionary)\n","```"]}],"metadata":{"colab":{"provenance":[{"file_id":"1DOXUNkzMGuhS4jc5zAYXBX9pW4TXf9Pi","timestamp":1674102012332}]},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"998d7648dd755371a7ba8fe881a42807a5528e9c57c793fb17dd78daafcc3440"}}},"nbformat":4,"nbformat_minor":0}
